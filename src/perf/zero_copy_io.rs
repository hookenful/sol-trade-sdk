//! ğŸš€ é›¶æ‹·è´å†…å­˜æ˜ å°„IO - å®Œå…¨æ¶ˆé™¤æ•°æ®æ‹·è´å¼€é”€
//! 
//! å®ç°æè‡´çš„é›¶æ‹·è´ç­–ç•¥ï¼ŒåŒ…æ‹¬ï¼š
//! - å†…å­˜æ˜ å°„æ–‡ä»¶IO
//! - å…±äº«å†…å­˜ç¯å½¢ç¼“å†²åŒº
//! - ç›´æ¥å†…å­˜è®¿é—®(DMA)æ¨¡æ‹Ÿ
//! - é›¶æ‹·è´ç½‘ç»œæ•°æ®ä¼ è¾“
//! - å†…å­˜æ± é¢„åˆ†é…ä¸é‡ç”¨

use std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};
use std::sync::Arc;
// use std::mem::{size_of, MaybeUninit};
use std::ptr::NonNull;
use std::slice;
use memmap2::{MmapMut, MmapOptions};
use anyhow::{Result, Context};
use crossbeam_utils::CachePadded;

/// ğŸš€ é›¶æ‹·è´å†…å­˜ç®¡ç†å™¨
pub struct ZeroCopyMemoryManager {
    /// å…±äº«å†…å­˜æ± 
    shared_pools: Vec<Arc<SharedMemoryPool>>,
    /// å†…å­˜æ˜ å°„ç¼“å†²åŒº
    mmap_buffers: Vec<Arc<MemoryMappedBuffer>>,
    /// ç›´æ¥å†…å­˜è®¿é—®ç®¡ç†å™¨
    dma_manager: Arc<DirectMemoryAccessManager>,
    /// ç»Ÿè®¡ä¿¡æ¯
    stats: Arc<ZeroCopyStats>,
}

/// ğŸš€ å…±äº«å†…å­˜æ±  - é¢„åˆ†é…å¤§å—å†…å­˜é¿å…è¿è¡Œæ—¶åˆ†é…
pub struct SharedMemoryPool {
    /// å†…å­˜æ˜ å°„åŒºåŸŸ
    memory_region: MmapMut,
    /// å¯ç”¨å—åˆ—è¡¨(ä½¿ç”¨ä½å›¾ç®¡ç†)
    free_blocks: Vec<AtomicU64>,
    /// å—å¤§å°
    block_size: usize,
    /// æ€»å—æ•°
    total_blocks: usize,
    /// åˆ†é…å™¨å¤´æŒ‡é’ˆ
    allocator_head: CachePadded<AtomicUsize>,
    /// æ± ID
    pool_id: u32,
}

impl SharedMemoryPool {
    /// åˆ›å»ºå…±äº«å†…å­˜æ± 
    pub fn new(pool_id: u32, total_size: usize, block_size: usize) -> Result<Self> {
        // ç¡®ä¿å—å¤§å°æ˜¯64å­—èŠ‚å¯¹é½(ç¼“å­˜è¡Œå¯¹é½)
        let aligned_block_size = (block_size + 63) & !63;
        let total_blocks = total_size / aligned_block_size;
        
        // åˆ›å»ºå†…å­˜æ˜ å°„æ–‡ä»¶
        let memory_region = MmapOptions::new()
            .len(total_blocks * aligned_block_size)
            .map_anon()
            .context("Failed to create memory mapped region")?;
        
        // åˆå§‹åŒ–ç©ºé—²å—ä½å›¾ (æ¯ä¸ªu64å¯ä»¥ç®¡ç†64ä¸ªå—)
        let bitmap_size = (total_blocks + 63) / 64;
        let mut free_blocks = Vec::with_capacity(bitmap_size);
        
        // å°†æ‰€æœ‰å—æ ‡è®°ä¸ºç©ºé—²(å…¨1)
        for i in 0..bitmap_size {
            let bits = if i == bitmap_size - 1 && total_blocks % 64 != 0 {
                // æœ€åä¸€ä¸ªu64å¯èƒ½ä¸æ»¡64ä½
                let valid_bits = total_blocks % 64;
                (1u64 << valid_bits) - 1
            } else {
                u64::MAX // æ‰€æœ‰64ä½éƒ½æ˜¯1
            };
            free_blocks.push(AtomicU64::new(bits));
        }
        
        tracing::info!(target: "sol_trade_sdk","ğŸš€ Created shared memory pool {} with {} blocks of {} bytes each", 
                  pool_id, total_blocks, aligned_block_size);
        
        Ok(Self {
            memory_region,
            free_blocks,
            block_size: aligned_block_size,
            total_blocks,
            allocator_head: CachePadded::new(AtomicUsize::new(0)),
            pool_id,
        })
    }
    
    /// ğŸš€ é›¶æ‹·è´åˆ†é…å†…å­˜å—
    #[inline(always)]
    pub fn allocate_block(&self) -> Option<ZeroCopyBlock> {
        // å¿«é€Ÿè·¯å¾„ï¼šå°è¯•ä»é¢„æœŸä½ç½®åˆ†é…
        let start_index = self.allocator_head.load(Ordering::Relaxed) / 64;
        
        // éå†æ‰€æœ‰ä½å›¾å¯»æ‰¾ç©ºé—²å—
        for attempt in 0..self.free_blocks.len() {
            let bitmap_index = (start_index + attempt) % self.free_blocks.len();
            let bitmap = &self.free_blocks[bitmap_index];
            
            let mut current = bitmap.load(Ordering::Acquire);
            
            while current != 0 {
                // æ‰¾åˆ°æœ€ä½ä½çš„1(æœ€å°çš„ç©ºé—²å—)
                let bit_pos = current.trailing_zeros() as usize;
                let mask = 1u64 << bit_pos;
                
                // å°è¯•åŸå­åœ°æ¸…é™¤è¿™ä¸€ä½(æ ‡è®°ä¸ºå·²åˆ†é…)
                match bitmap.compare_exchange_weak(
                    current, 
                    current & !mask,
                    Ordering::AcqRel,
                    Ordering::Relaxed
                ) {
                    Ok(_) => {
                        // æˆåŠŸåˆ†é…
                        let block_index = bitmap_index * 64 + bit_pos;
                        if block_index >= self.total_blocks {
                            // è¶…å‡ºè¾¹ç•Œï¼Œæ¢å¤ä½å¹¶ç»§ç»­
                            bitmap.fetch_or(mask, Ordering::Relaxed);
                            break;
                        }
                        
                        let offset = block_index * self.block_size;
                        let ptr = unsafe {
                            NonNull::new_unchecked(
                                self.memory_region.as_ptr().add(offset) as *mut u8
                            )
                        };
                        
                        // æ›´æ–°åˆ†é…å™¨å¤´æŒ‡é’ˆ
                        self.allocator_head.store(
                            (block_index + 1) * 64, 
                            Ordering::Relaxed
                        );
                        
                        return Some(ZeroCopyBlock {
                            ptr,
                            size: self.block_size,
                            pool_id: self.pool_id,
                            block_index,
                        });
                    }
                    Err(new_current) => {
                        current = new_current;
                        continue;
                    }
                }
            }
        }
        
        None // æ²¡æœ‰å¯ç”¨å—
    }
    
    /// ğŸš€ é›¶æ‹·è´é‡Šæ”¾å†…å­˜å—
    #[inline(always)]
    pub fn deallocate_block(&self, block: ZeroCopyBlock) {
        if block.pool_id != self.pool_id {
            tracing::error!(target: "sol_trade_sdk", "Attempting to deallocate block from wrong pool");
            return;
        }
        
        let bitmap_index = block.block_index / 64;
        let bit_pos = block.block_index % 64;
        let mask = 1u64 << bit_pos;
        
        if bitmap_index < self.free_blocks.len() {
            // åŸå­åœ°è®¾ç½®ä½ä¸º1(æ ‡è®°ä¸ºç©ºé—²)
            self.free_blocks[bitmap_index].fetch_or(mask, Ordering::Release);
        }
    }
    
    /// è·å–å¯ç”¨å—æ•°é‡
    pub fn available_blocks(&self) -> usize {
        self.free_blocks.iter()
            .map(|bitmap| bitmap.load(Ordering::Relaxed).count_ones() as usize)
            .sum()
    }
}

/// ğŸš€ é›¶æ‹·è´å†…å­˜å—
pub struct ZeroCopyBlock {
    /// å†…å­˜æŒ‡é’ˆ
    ptr: NonNull<u8>,
    /// å—å¤§å°
    size: usize,
    /// æ‰€å±æ± ID
    pool_id: u32,
    /// å—ç´¢å¼•
    block_index: usize,
}

impl ZeroCopyBlock {
    /// è·å–å†…å­˜æŒ‡é’ˆ
    #[inline(always)]
    pub fn as_ptr(&self) -> *mut u8 {
        self.ptr.as_ptr()
    }
    
    /// è·å–åªè¯»åˆ‡ç‰‡
    #[inline(always)]
    pub unsafe fn as_slice(&self) -> &[u8] {
        slice::from_raw_parts(self.ptr.as_ptr(), self.size)
    }
    
    /// è·å–å¯å˜åˆ‡ç‰‡
    #[inline(always)]
    pub unsafe fn as_mut_slice(&mut self) -> &mut [u8] {
        slice::from_raw_parts_mut(self.ptr.as_ptr(), self.size)
    }
    
    /// è·å–å—å¤§å°
    #[inline(always)]
    pub fn size(&self) -> usize {
        self.size
    }
    
    /// é›¶æ‹·è´å†™å…¥æ•°æ®
    #[inline(always)]
    pub unsafe fn write_bytes(&mut self, data: &[u8]) -> Result<()> {
        if data.len() > self.size {
            return Err(anyhow::anyhow!("Data too large for block"));
        }
        
        // ä½¿ç”¨ç¡¬ä»¶ä¼˜åŒ–çš„å†…å­˜æ‹·è´
        super::hardware_optimizations::SIMDMemoryOps::memcpy_simd_optimized(
            self.ptr.as_ptr(),
            data.as_ptr(),
            data.len()
        );
        
        Ok(())
    }
    
    /// é›¶æ‹·è´è¯»å–æ•°æ®
    #[inline(always)]
    pub unsafe fn read_bytes(&self, len: usize) -> Result<&[u8]> {
        if len > self.size {
            return Err(anyhow::anyhow!("Read length exceeds block size"));
        }
        
        Ok(slice::from_raw_parts(self.ptr.as_ptr(), len))
    }
}

unsafe impl Send for ZeroCopyBlock {}
unsafe impl Sync for ZeroCopyBlock {}

/// ğŸš€ å†…å­˜æ˜ å°„ç¼“å†²åŒº - å¤§æ•°æ®é›¶æ‹·è´ä¼ è¾“
pub struct MemoryMappedBuffer {
    /// å†…å­˜æ˜ å°„åŒºåŸŸ
    mmap: MmapMut,
    /// è¯»æŒ‡é’ˆ
    read_pos: CachePadded<AtomicUsize>,
    /// å†™æŒ‡é’ˆ
    write_pos: CachePadded<AtomicUsize>,
    /// ç¼“å†²åŒºå¤§å°
    size: usize,
    /// ç¼“å†²åŒºID
    _buffer_id: u64,
}

impl MemoryMappedBuffer {
    /// åˆ›å»ºå†…å­˜æ˜ å°„ç¼“å†²åŒº
    pub fn new(buffer_id: u64, size: usize) -> Result<Self> {
        let mmap = MmapOptions::new()
            .len(size)
            .map_anon()
            .context("Failed to create memory mapped buffer")?;
        
        tracing::info!(target: "sol_trade_sdk","ğŸš€ Created memory mapped buffer {} with size {} bytes", buffer_id, size);
        
        Ok(Self {
            mmap,
            read_pos: CachePadded::new(AtomicUsize::new(0)),
            write_pos: CachePadded::new(AtomicUsize::new(0)),
            size,
            _buffer_id: buffer_id,
        })
    }
    
    /// ğŸš€ é›¶æ‹·è´å†™å…¥æ•°æ®
    #[inline(always)]
    pub fn write_data(&self, data: &[u8]) -> Result<usize> {
        let data_len = data.len();
        let current_write = self.write_pos.load(Ordering::Relaxed);
        let current_read = self.read_pos.load(Ordering::Acquire);
        
        // è®¡ç®—å¯ç”¨ç©ºé—´
        let available_space = if current_write >= current_read {
            self.size - (current_write - current_read) - 1
        } else {
            current_read - current_write - 1
        };
        
        if data_len > available_space {
            return Err(anyhow::anyhow!("Insufficient buffer space"));
        }
        
        // é›¶æ‹·è´å†™å…¥
        unsafe {
            let write_ptr = self.mmap.as_ptr().add(current_write) as *mut u8;
            
            if current_write + data_len <= self.size {
                // æ•°æ®ä¸è·¨è¶Šç¼“å†²åŒºè¾¹ç•Œ
                super::hardware_optimizations::SIMDMemoryOps::memcpy_simd_optimized(
                    write_ptr, data.as_ptr(), data_len
                );
            } else {
                // æ•°æ®è·¨è¶Šç¼“å†²åŒºè¾¹ç•Œï¼Œåˆ†ä¸¤æ®µå†™å…¥
                let first_part = self.size - current_write;
                let second_part = data_len - first_part;
                
                // å†™å…¥ç¬¬ä¸€éƒ¨åˆ†
                super::hardware_optimizations::SIMDMemoryOps::memcpy_simd_optimized(
                    write_ptr, data.as_ptr(), first_part
                );
                
                // å†™å…¥ç¬¬äºŒéƒ¨åˆ†(ä»ç¼“å†²åŒºå¼€å¤´)
                super::hardware_optimizations::SIMDMemoryOps::memcpy_simd_optimized(
                    self.mmap.as_ptr() as *mut u8, 
                    data.as_ptr().add(first_part), 
                    second_part
                );
            }
        }
        
        // æ›´æ–°å†™æŒ‡é’ˆ
        let new_write_pos = (current_write + data_len) % self.size;
        self.write_pos.store(new_write_pos, Ordering::Release);
        
        Ok(data_len)
    }
    
    /// ğŸš€ é›¶æ‹·è´è¯»å–æ•°æ®
    #[inline(always)]
    pub fn read_data(&self, buffer: &mut [u8]) -> Result<usize> {
        let buffer_len = buffer.len();
        let current_read = self.read_pos.load(Ordering::Relaxed);
        let current_write = self.write_pos.load(Ordering::Acquire);
        
        // è®¡ç®—å¯è¯»æ•°æ®é‡
        let available_data = if current_write >= current_read {
            current_write - current_read
        } else {
            self.size - (current_read - current_write)
        };
        
        if available_data == 0 {
            return Ok(0); // æ— æ•°æ®å¯è¯»
        }
        
        let read_len = buffer_len.min(available_data);
        
        // é›¶æ‹·è´è¯»å–
        unsafe {
            let read_ptr = self.mmap.as_ptr().add(current_read);
            
            if current_read + read_len <= self.size {
                // æ•°æ®ä¸è·¨è¶Šç¼“å†²åŒºè¾¹ç•Œ
                super::hardware_optimizations::SIMDMemoryOps::memcpy_simd_optimized(
                    buffer.as_mut_ptr(), read_ptr, read_len
                );
            } else {
                // æ•°æ®è·¨è¶Šç¼“å†²åŒºè¾¹ç•Œï¼Œåˆ†ä¸¤æ®µè¯»å–
                let first_part = self.size - current_read;
                let second_part = read_len - first_part;
                
                // è¯»å–ç¬¬ä¸€éƒ¨åˆ†
                super::hardware_optimizations::SIMDMemoryOps::memcpy_simd_optimized(
                    buffer.as_mut_ptr(), read_ptr, first_part
                );
                
                // è¯»å–ç¬¬äºŒéƒ¨åˆ†(ä»ç¼“å†²åŒºå¼€å¤´)
                super::hardware_optimizations::SIMDMemoryOps::memcpy_simd_optimized(
                    buffer.as_mut_ptr().add(first_part),
                    self.mmap.as_ptr(), 
                    second_part
                );
            }
        }
        
        // æ›´æ–°è¯»æŒ‡é’ˆ
        let new_read_pos = (current_read + read_len) % self.size;
        self.read_pos.store(new_read_pos, Ordering::Release);
        
        Ok(read_len)
    }
    
    /// è·å–å¯è¯»æ•°æ®é‡
    #[inline(always)]
    pub fn available_data(&self) -> usize {
        let current_read = self.read_pos.load(Ordering::Relaxed);
        let current_write = self.write_pos.load(Ordering::Relaxed);
        
        if current_write >= current_read {
            current_write - current_read
        } else {
            self.size - (current_read - current_write)
        }
    }
    
    /// è·å–å¯ç”¨ç©ºé—´
    #[inline(always)]
    pub fn available_space(&self) -> usize {
        self.size - self.available_data() - 1
    }
}

/// ğŸš€ ç›´æ¥å†…å­˜è®¿é—®ç®¡ç†å™¨ - æ¨¡æ‹ŸDMAæ“ä½œ
pub struct DirectMemoryAccessManager {
    /// DMAé€šé“æ± 
    dma_channels: Vec<Arc<DMAChannel>>,
    /// é€šé“åˆ†é…å™¨
    channel_allocator: AtomicUsize,
    /// ç»Ÿè®¡ä¿¡æ¯
    dma_stats: Arc<DMAStats>,
}

impl DirectMemoryAccessManager {
    /// åˆ›å»ºDMAç®¡ç†å™¨
    pub fn new(num_channels: usize) -> Result<Self> {
        let mut dma_channels = Vec::with_capacity(num_channels);
        
        for i in 0..num_channels {
            dma_channels.push(Arc::new(DMAChannel::new(i)?));
        }
        
        tracing::info!(target: "sol_trade_sdk","ğŸš€ Created DMA manager with {} channels", num_channels);
        
        Ok(Self {
            dma_channels,
            channel_allocator: AtomicUsize::new(0),
            dma_stats: Arc::new(DMAStats::new()),
        })
    }
    
    /// ğŸš€ æ‰§è¡Œé›¶æ‹·è´DMAä¼ è¾“
    #[inline(always)]
    pub async fn dma_transfer(&self, src: &[u8], dst: &mut [u8]) -> Result<usize> {
        if src.len() != dst.len() {
            return Err(anyhow::anyhow!("Source and destination sizes don't match"));
        }
        
        // é€‰æ‹©DMAé€šé“(è½®è¯¢åˆ†é…)
        let channel_index = self.channel_allocator.fetch_add(1, Ordering::Relaxed) % self.dma_channels.len();
        let channel = &self.dma_channels[channel_index];
        
        // æ‰§è¡ŒDMAä¼ è¾“
        let transferred = channel.transfer(src, dst).await?;
        
        // æ›´æ–°ç»Ÿè®¡
        self.dma_stats.bytes_transferred.fetch_add(transferred as u64, Ordering::Relaxed);
        self.dma_stats.transfers_completed.fetch_add(1, Ordering::Relaxed);
        
        Ok(transferred)
    }
}

/// ğŸš€ DMAé€šé“
pub struct DMAChannel {
    /// é€šé“ID
    _channel_id: usize,
    /// ä¼ è¾“é˜Ÿåˆ—
    _transfer_queue: crossbeam_queue::ArrayQueue<DMATransfer>,
    /// é€šé“çŠ¶æ€
    _status: AtomicU64,
}

impl DMAChannel {
    /// åˆ›å»ºDMAé€šé“
    pub fn new(channel_id: usize) -> Result<Self> {
        Ok(Self {
            _channel_id: channel_id,
            _transfer_queue: crossbeam_queue::ArrayQueue::new(1024),
            _status: AtomicU64::new(0),
        })
    }
    
    /// ğŸš€ æ‰§è¡Œé›¶æ‹·è´ä¼ è¾“
    #[inline(always)]
    pub async fn transfer(&self, src: &[u8], dst: &mut [u8]) -> Result<usize> {
        let transfer_size = src.len();
        
        // ä½¿ç”¨ç¡¬ä»¶ä¼˜åŒ–çš„SIMDå†…å­˜æ‹·è´
        unsafe {
            super::hardware_optimizations::SIMDMemoryOps::memcpy_simd_optimized(
                dst.as_mut_ptr(),
                src.as_ptr(),
                transfer_size
            );
        }
        
        Ok(transfer_size)
    }
}

/// DMAä¼ è¾“æè¿°ç¬¦
#[derive(Debug)]
pub struct DMATransfer {
    pub src_addr: usize,
    pub dst_addr: usize,
    pub size: usize,
    pub flags: u32,
}

/// DMAç»Ÿè®¡ä¿¡æ¯
pub struct DMAStats {
    pub bytes_transferred: AtomicU64,
    pub transfers_completed: AtomicU64,
    pub transfer_errors: AtomicU64,
}

impl DMAStats {
    pub fn new() -> Self {
        Self {
            bytes_transferred: AtomicU64::new(0),
            transfers_completed: AtomicU64::new(0),
            transfer_errors: AtomicU64::new(0),
        }
    }
}

/// ğŸš€ é›¶æ‹·è´ç»Ÿè®¡ä¿¡æ¯
pub struct ZeroCopyStats {
    /// åˆ†é…çš„å—æ•°
    pub blocks_allocated: AtomicU64,
    /// é‡Šæ”¾çš„å—æ•°
    pub blocks_freed: AtomicU64,
    /// é›¶æ‹·è´ä¼ è¾“å­—èŠ‚æ•°
    pub bytes_transferred: AtomicU64,
    /// å†…å­˜æ˜ å°„ç¼“å†²åŒºä½¿ç”¨é‡
    pub mmap_buffer_usage: AtomicU64,
}

impl ZeroCopyStats {
    pub fn new() -> Self {
        Self {
            blocks_allocated: AtomicU64::new(0),
            blocks_freed: AtomicU64::new(0),
            bytes_transferred: AtomicU64::new(0),
            mmap_buffer_usage: AtomicU64::new(0),
        }
    }
    
    /// æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    pub fn print_stats(&self) {
        let allocated = self.blocks_allocated.load(Ordering::Relaxed);
        let freed = self.blocks_freed.load(Ordering::Relaxed);
        let bytes = self.bytes_transferred.load(Ordering::Relaxed);
        let mmap_usage = self.mmap_buffer_usage.load(Ordering::Relaxed);
        
        tracing::info!(target: "sol_trade_sdk","ğŸš€ Zero-Copy Stats:");
        tracing::info!(target: "sol_trade_sdk","   ğŸ“¦ Blocks: Allocated={}, Freed={}, Active={}", 
                  allocated, freed, allocated.saturating_sub(freed));
        tracing::info!(target: "sol_trade_sdk","   ğŸ“Š Bytes Transferred: {} ({:.2} MB)", 
                  bytes, bytes as f64 / 1024.0 / 1024.0);
        tracing::info!(target: "sol_trade_sdk","   ğŸ’¾ Memory Mapped Usage: {} ({:.2} MB)", 
                  mmap_usage, mmap_usage as f64 / 1024.0 / 1024.0);
    }
}

impl ZeroCopyMemoryManager {
    /// åˆ›å»ºé›¶æ‹·è´å†…å­˜ç®¡ç†å™¨
    pub fn new() -> Result<Self> {
        let mut shared_pools = Vec::new();
        let mut mmap_buffers = Vec::new();
        
        // åˆ›å»ºä¸åŒå¤§å°çš„å†…å­˜æ± 
        // å°å—æ± : 64KB blocks, 1GB total
        shared_pools.push(Arc::new(SharedMemoryPool::new(0, 1024 * 1024 * 1024, 64 * 1024)?));
        // ä¸­å—æ± : 1MB blocks, 4GB total  
        shared_pools.push(Arc::new(SharedMemoryPool::new(1, 4 * 1024 * 1024 * 1024, 1024 * 1024)?));
        // å¤§å—æ± : 16MB blocks, 8GB total
        shared_pools.push(Arc::new(SharedMemoryPool::new(2, 8 * 1024 * 1024 * 1024, 16 * 1024 * 1024)?));
        
        // åˆ›å»ºå†…å­˜æ˜ å°„ç¼“å†²åŒº
        for i in 0..8 {
            mmap_buffers.push(Arc::new(MemoryMappedBuffer::new(i, 256 * 1024 * 1024)?)); // 256MB each
        }
        
        let dma_manager = Arc::new(DirectMemoryAccessManager::new(16)?); // 16 DMA channels
        let stats = Arc::new(ZeroCopyStats::new());
        
        tracing::info!(target: "sol_trade_sdk","ğŸš€ Zero-Copy Memory Manager initialized");
        tracing::info!(target: "sol_trade_sdk","   ğŸ“¦ Memory Pools: {}", shared_pools.len());
        tracing::info!(target: "sol_trade_sdk","   ğŸ’¾ Mapped Buffers: {}", mmap_buffers.len());
        tracing::info!(target: "sol_trade_sdk","   ğŸ”„ DMA Channels: 16");
        
        Ok(Self {
            shared_pools,
            mmap_buffers,
            dma_manager,
            stats,
        })
    }
    
    /// ğŸš€ åˆ†é…é›¶æ‹·è´å†…å­˜å—
    #[inline(always)]
    pub fn allocate(&self, size: usize) -> Option<ZeroCopyBlock> {
        // æ ¹æ®å¤§å°é€‰æ‹©åˆé€‚çš„å†…å­˜æ± 
        let pool = if size <= 64 * 1024 {
            &self.shared_pools[0] // å°å—æ± 
        } else if size <= 1024 * 1024 {
            &self.shared_pools[1] // ä¸­å—æ± 
        } else {
            &self.shared_pools[2] // å¤§å—æ± 
        };
        
        if let Some(block) = pool.allocate_block() {
            self.stats.blocks_allocated.fetch_add(1, Ordering::Relaxed);
            Some(block)
        } else {
            None
        }
    }
    
    /// ğŸš€ é‡Šæ”¾é›¶æ‹·è´å†…å­˜å—
    #[inline(always)]
    pub fn deallocate(&self, block: ZeroCopyBlock) {
        let pool_id = block.pool_id as usize;
        if pool_id < self.shared_pools.len() {
            self.shared_pools[pool_id].deallocate_block(block);
            self.stats.blocks_freed.fetch_add(1, Ordering::Relaxed);
        }
    }
    
    /// è·å–å†…å­˜æ˜ å°„ç¼“å†²åŒº
    #[inline(always)]
    pub fn get_mmap_buffer(&self, buffer_id: usize) -> Option<Arc<MemoryMappedBuffer>> {
        self.mmap_buffers.get(buffer_id).cloned()
    }
    
    /// è·å–DMAç®¡ç†å™¨
    #[inline(always)]
    pub fn get_dma_manager(&self) -> Arc<DirectMemoryAccessManager> {
        self.dma_manager.clone()
    }
    
    /// è·å–ç»Ÿè®¡ä¿¡æ¯
    pub fn get_stats(&self) -> Arc<ZeroCopyStats> {
        self.stats.clone()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_shared_memory_pool() -> Result<()> {
        let pool = SharedMemoryPool::new(0, 1024 * 1024, 4096)?;
        
        // æµ‹è¯•åˆ†é…
        let block1 = pool.allocate_block().expect("Should allocate block");
        assert_eq!(block1.size(), 4096);
        
        let block2 = pool.allocate_block().expect("Should allocate another block");
        assert_eq!(block2.size(), 4096);
        
        // æµ‹è¯•é‡Šæ”¾
        pool.deallocate_block(block1);
        pool.deallocate_block(block2);
        
        Ok(())
    }
    
    #[tokio::test]
    async fn test_memory_mapped_buffer() -> Result<()> {
        let buffer = MemoryMappedBuffer::new(0, 1024 * 1024)?;
        
        let test_data = b"Hello, Zero-Copy World!";
        
        // æµ‹è¯•å†™å…¥
        let written = buffer.write_data(test_data)?;
        assert_eq!(written, test_data.len());
        
        // æµ‹è¯•è¯»å–
        let mut read_buffer = vec![0u8; test_data.len()];
        let read = buffer.read_data(&mut read_buffer)?;
        assert_eq!(read, test_data.len());
        assert_eq!(&read_buffer, test_data);
        
        Ok(())
    }
    
    #[tokio::test]
    async fn test_dma_transfer() -> Result<()> {
        let dma_manager = DirectMemoryAccessManager::new(4)?;
        
        let src = vec![1u8, 2, 3, 4, 5, 6, 7, 8];
        let mut dst = vec![0u8; 8];
        
        let transferred = dma_manager.dma_transfer(&src, &mut dst).await?;
        assert_eq!(transferred, 8);
        assert_eq!(src, dst);
        
        Ok(())
    }
    
    #[tokio::test]
    async fn test_zero_copy_manager() -> Result<()> {
        let manager = ZeroCopyMemoryManager::new()?;
        
        // æµ‹è¯•å°å—åˆ†é…
        let small_block = manager.allocate(1024).expect("Should allocate small block");
        assert_eq!(small_block.size(), 65536); // å°å—æ± çš„å—å¤§å°
        
        // æµ‹è¯•å¤§å—åˆ†é…
        let large_block = manager.allocate(5 * 1024 * 1024).expect("Should allocate large block");
        assert_eq!(large_block.size(), 16 * 1024 * 1024); // å¤§å—æ± çš„å—å¤§å°
        
        manager.deallocate(small_block);
        manager.deallocate(large_block);
        
        Ok(())
    }
}